
1、重要概念
    NAS:网络存储
    SAN:存储区域网络

2、分布式存储与NAS、SAN容错思路：
    NAS、SAN采用双控系统容错思路，也就是传说中的硬件冗余
    分布式系统采用搭建多服务器系统使整体达到高可靠性

3、数据完整性使数据存储系统的基石

4、典型故障场景及解决方法

4.1、客户端无法定位服务器
    1）域名故障场景
        添加域名缓存
    2）域名记录更新导致本地缓存无效
        设置域名缓存生命周期

4.2、客户端请求丢失
    1）网络质量差导致请求丢失
        客户端设置超时重传机制
    2）网络堵塞导致请求丢失
        重传请求的指数回退，或者选择合适的网络协议盏进行优化
    3）传输大对象时部分请求丢失
        断点续传
    4）客户端重传请求导致服务端重复处理请求
        服务端的幂等性

4.3、服务端收到请求后崩溃
    1）服务端的解决方法为采用事务机制，客户端为请求分配ID，服务端在处理的时候采用事务的方式处理请求并记录ID值
        1、事务执行时崩溃
            此时客户端会超时，然后重新发送请求直到服务端恢复，恢复时进行清理动作保证事务正确性，服务端重新进入正常状态后根据请求ID检查到事务还未执行，则先执行事务然后返回响应
        2、事务执行后崩溃
            此时客户端会超时，然后重新发送请求直到服务端恢复，恢复时进行清理动作保证事务正确性，服务端重新进入正常状态后根据请求ID检查到事务已经执行，则返回响应
    2）客户端的解决方案为请求超时重试机制

4.4、服务端响应丢失
    服务端处理成功，并返回响应后，响应在给客户端的传输过程中丢失此时需要客户端超时重发并做好ID分配

4.5、客户端崩溃
    当客户端成功发出请求后，因为某种原因崩溃可能发生如下情况：
    1）客户端请求丢失，影响较小、后果可控
    2）服务端崩溃，可能产生垃圾数据
    3）响应丢失，产生垃圾数据

5、分布式提交技术(pc)
    对于分布式数据存储系统来说，实现原子提交协议是在数据写入时实现系统容错的关键技术，典型的原子提交技术包含两阶段提交(two phase commit)、三阶段提交(three phase commit)

5.1、两阶段提交(2pc)
    1）两阶段包含协调者和参与者，协调者负责发起请求，参与者负责执行请求
    2）核心思想是将请求的数据资源准备和最终提交分两个阶段
        第一阶段为准备阶段(prepare)，此时协调者让所有参与者准备好资源，然后参与者返回已准备(该信息可能包含成功，也可能包含失败，还有可能超时)
        第二阶段为提交/回退阶段，协调者根据所有已返回的信息进行决策，如果绝大多数的参与者都准备好了则提交，否则回退

    3）两阶段提交仅仅解决了参与者故障的问题并没有解决协调者故障的问题，因此三次提交应运而生

5.2、三阶段提交(3pc)
    1）3pc在2pc的基础上增加判断是否可以提交(canCommit)阶段，
    2）该阶段用来支持在旧协调者发生故障后，新协调者重新发起请求时的资源清理，该阶段正常执行后就可以按照2pc执行了

6、日志恢复技术
    支持容错的分布式数据存储系统在服务器发生故障后需要执行清理和恢复工作，并且将参与者恢复到未执行请求前的正常状态
    容错时的清理和恢复工作都需要使用日志恢复技术

    日志恢复算法(ARIES)包含写前日志(wal)、重做日志(redo)和回滚日志(undo)

6.1、记录日志
    为了保证完整恢复，采用写前日志技术，也就是说，写请求真正修改数据之前需要将所有改变信息写入日志，并且确保日志持久化保存
    因此当数据存储系统写入真实对象出现故障时，也可以通过写前日志完成恢复，若写入真实对象成功，则做好后续的日志清理工作即可

6.2、数据恢复
    数据恢复主要包含如下三个阶段
    1）分析
        服务器重启之后根据写前日志分析服务器故障时间点软件的执行状态，然后根据分析结果确定具体的动作和步骤
    2）重做
        经过分析阶段的输出，根据写前日志执行重做请求，将服务器恢复到故障时间点的状态，特别时在内存中构建出故障时间点的运行状态
    3）回滚
        结合分析阶段的输出，某些请求无法继续执行，因此需要将它们清理干净，此时就需要根据写前日志执行回滚

6.3、检查点
    主要时为了减少重做或回滚的数据量，因为存储系统长时间运行后会产生大量的wal，如果从头分析日志则时间会很长
    检查点之前的wal已经全部执行完毕，之后的数据时未执行的数据，如果此时系统崩溃则只需要分析检查点之后的wal即可

7、对象存储容错
    公共云对象存储服务更关心如下的能力：
    1）故障检测及时性
    2）故障原因分析的准确性
    3）故障恢复高效性

8、数据完整性
    数据损坏可以带来数据完整性问题，数据损坏的故障可以通过容错技术进行修复，从而保证数据完整性
    典型的数据损坏包含数据丢失、数据反转

8.1、数据损坏按检测维度分为两类：
    1）已检测损坏
    2）未检测损坏
        也叫做静默数据错误，是影响数据完整性的最大挑战

8.2、数据完整性
    1）物理完整性
        更多的强调硬件故障，实现物理完整性的方法纠删码、副本集等技术
    2）逻辑完整性
        强调业务、应用的数据正确性、合理性，主要涞源于程序bug，修复难度比较大

8.3、数据损坏的源头
    1）磁盘介质的位翻转
    2）内存位翻转
    3）网卡位翻转
    4）CPU计算错误
    5）软件bug

8.4、数据损坏类型
    1）应用数据损坏
    2）应用元数据损坏
    从物理完整性角度看可以通过纠删码的方式解决，从逻辑完整性看要通过业务、应用层的保护机制解决，比如数据复制和备份

8.5、数据损坏发生时刻
    1）运行时刻
    2）静态时刻

8.6、数据损坏检测方法
    1）前台检测
        在数据请求的io流中检测
    2）后台检测
        在数据持久化保存后进行检测，此时需要定期的后台扫描发现数据损坏

8.7、数据损坏的检测算法
    1）异或算法(XOR)
    2）循环冗余校验(CRC)
    3）纠删码(ECC)

8.8、数据损坏修复技术
    1）数据冗余修复
    2）数据备份修复

8.9、对象存储数据检测技术
    1）数据上传服务器时，在客户端计算出CRC校验值，上传完毕后对象存储服务会返回服务端计算的CRC校验值，通过客户端对比，判断是否存在网络错误
    2）服务端做数据校验
    3）针对元数据做校验
    4）存储层提供数据校验，比如raid技术




